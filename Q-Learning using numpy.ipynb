{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Accompanying notebook for the FloydHub article: \"An introduction to Q-Learning: Reinforcement Learning\""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](https://i.ibb.co/c8LXj7X/Capture.png)\n\n<center>The environment</center>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Only numpy\nimport numpy as np",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Initialize parameters\ngamma = 0.75 # Discount factor \nalpha = 0.9 # Learning rate ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define the states\nlocation_to_state = {\n    'L1' : 0,\n    'L2' : 1,\n    'L3' : 2,\n    'L4' : 3,\n    'L5' : 4,\n    'L6' : 5,\n    'L7' : 6,\n    'L8' : 7,\n    'L9' : 8\n}",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define the actions\nactions = [0,1,2,3,4,5,6,7,8]",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](https://i.ibb.co/k4kgnQS/Capture.png)\n\n<center>Rewards' table</center>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define the rewards\nrewards = np.array([[0,1,0,0,0,0,0,0,0],\n              [1,0,1,0,0,0,0,0,0],\n              [0,1,0,0,0,1,0,0,0],\n              [0,0,0,0,0,0,1,0,0],\n              [0,1,0,0,0,0,0,1,0],\n              [0,0,1,0,0,0,0,0,0],\n              [0,0,0,1,0,0,0,1,0],\n              [0,0,0,0,1,0,1,0,1],\n              [0,0,0,0,0,0,0,1,0]])",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Maps indices to locations\nstate_to_location = dict((state,location) for location,state in location_to_state.items())",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Q-Learning parameters\ngamma = 0.75 # Discount factor for temporal difference\nalpha = 0.9 # Learning rate ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define the states\nlocation_to_state = {\n    'L1' : 0,\n    'L2' : 1,\n    'L3' : 2,\n    'L4' : 3,\n    'L5' : 4,\n    'L6' : 5,\n    'L7' : 6,\n    'L8' : 7,\n    'L9' : 8\n}",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define the actions\nactions = [0,1,2,3,4,5,6,7,8]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define the rewards\nrewards = np.array([[0,1,0,0,0,0,0,0,0],\n                  [1,0,1,0,0,0,0,0,0],\n                  [0,1,0,0,0,1,0,0,0],\n                  [0,0,0,0,0,0,1,0,0],\n                  [0,1,0,0,0,0,0,1,0],\n                  [0,0,1,0,0,0,0,0,0],\n                  [0,0,0,1,0,0,0,1,0],\n                  [0,0,0,0,1,0,1,0,1],\n                  [0,0,0,0,0,0,0,1,0]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Maps indices to locations\nstate_to_location = {state : location for location, state in location_to_state.items()}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following function is going to take two arguments: \n\n- starting location in the warehouse and \n- end location in the warehouse respectively \n\nIt will return the optimal route for reaching the end location from the starting location in the form of an ordered list (containing the letters)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_optimal_route(start_location,end_location):\n    # Copy the rewards matrix to new Matrix\n    rewards_new = np.copy(rewards)\n    # Get the ending state corresponding to the ending location as given\n    ending_state = location_to_state[end_location]\n    # With the above information automatically set the priority of the given ending state to the highest one\n    rewards_new[ending_state,ending_state] = 999\n\n    # -----------Q-Learning algorithm-----------\n   \n    # Initializing Q-Values\n    Q = np.array(np.zeros([9,9]))\n\n    # Q-Learning process\n    for i in range(1000):\n        # Pick up a state randomly\n        current_state = np.random.randint(0,9) # Python excludes the upper bound\n        # For traversing through the neighbor locations in the maze\n        playable_actions = []\n        # Iterate through the new rewards matrix and get the actions > 0\n        for j in range(9):\n            if rewards_new[current_state,j] > 0:\n                playable_actions.append(j)\n        # Pick an action randomly from the list of playable actions leading us to the next state\n        next_state = np.random.choice(playable_actions)\n        # Compute the temporal difference\n        # The action here exactly refers to going to the next state\n        TD = rewards_new[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]\n        # Update the Q-Value using the Bellman equation\n        Q[current_state,next_state] += alpha * TD\n\n    # Initialize the optimal route with the starting location\n    route = [start_location]\n    # We do not know about the next location yet, so initialize with the value of starting location\n    next_location = start_location\n    \n    # We don't know about the exact number of iterations needed to reach to the final location hence while loop will be a good choice for iteratiing\n    while(next_location != end_location):\n        # Fetch the starting state\n        starting_state = location_to_state[start_location]\n        # Fetch the highest Q-value pertaining to starting state\n        next_state = np.argmax(Q[starting_state,])\n        # We got the index of the next state. But we need the corresponding letter. \n        next_location = state_to_location[next_state]\n        route.append(next_location)\n        # Update the starting location for the next iteration\n        start_location = next_location\n    \n    return route",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "get_optimal_route('L9', 'L1')",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 34,
          "data": {
            "text/plain": "['L9', 'L8', 'L5', 'L2', 'L1']"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}